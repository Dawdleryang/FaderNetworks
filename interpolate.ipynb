{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-attribute swaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First train an autoencoder using the following parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "INFO - 05/28/18 19:04:05 - 0:00:00 - ============ Initialized logger ============\n",
    "INFO - 05/28/18 19:04:05 - 0:00:00 - ae_optimizer: adam,lr=0.0002\n",
    "                                     ae_reload: \n",
    "                                     attr: [('Male', 2), ('Smiling', 2)]\n",
    "                                     batch_size: 32\n",
    "                                     clf_dis_reload: \n",
    "                                     clip_grad_norm: 5\n",
    "                                     debug: False\n",
    "                                     dec_dropout: 0.0\n",
    "                                     deconv_method: convtranspose\n",
    "                                     dis_optimizer: adam,lr=0.0002\n",
    "                                     dump_path: /home/ubuntu/FaderNetworks/models/default/paoucvpr8a\n",
    "                                     epoch_size: 50000\n",
    "                                     eval_clf: models/classifier256.pth\n",
    "                                     h_flip: True\n",
    "                                     hid_dim: 512\n",
    "                                     img_fm: 3\n",
    "                                     img_sz: 256\n",
    "                                     init_fm: 32\n",
    "                                     instance_norm: False\n",
    "                                     lambda_ae: 1\n",
    "                                     lambda_clf_dis: 0\n",
    "                                     lambda_lat_dis: 0.0001\n",
    "                                     lambda_ptc_dis: 0\n",
    "                                     lambda_schedule: 500000\n",
    "                                     lat_dis_dropout: 0.3\n",
    "                                     lat_dis_reload: \n",
    "                                     max_fm: 512\n",
    "                                     n_attr: 4\n",
    "                                     n_clf_dis: 0\n",
    "                                     n_epochs: 300\n",
    "                                     n_lat_dis: 1\n",
    "                                     n_layers: 6\n",
    "                                     n_ptc_dis: 0\n",
    "                                     n_skip: 0\n",
    "                                     name: default\n",
    "                                     ptc_dis_reload: \n",
    "                                     smooth_label: 0.2\n",
    "                                     v_flip: False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '/home/ubuntu/FaderNetworks/models/default/paoucvpr8a/best_rec_ae.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/serialization.py:325: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "ae = torch.load(MODEL_PATH).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.logger import create_logger\n",
    "from src.loader import load_images, DataSampler\n",
    "from src.utils import bool_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--output_path'], dest='output_path', nargs=None, const=None, default='output.png', type=<class 'str'>, choices=None, help='Output path', metavar=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Attributes swapping')\n",
    "parser.add_argument(\"--model_path\", type=str, default=\"\",\n",
    "                    help=\"Trained model path\")\n",
    "parser.add_argument(\"--n_images\", type=int, default=10,\n",
    "                    help=\"Number of images to modify\")\n",
    "parser.add_argument(\"--offset\", type=int, default=0,\n",
    "                    help=\"First image index\")\n",
    "parser.add_argument(\"--n_interpolations\", type=int, default=10,\n",
    "                    help=\"Number of interpolations per image\")\n",
    "parser.add_argument(\"--alpha_min\", type=float, default=1,\n",
    "                    help=\"Min interpolation value\")\n",
    "parser.add_argument(\"--alpha_max\", type=float, default=1,\n",
    "                    help=\"Max interpolation value\")\n",
    "parser.add_argument(\"--plot_size\", type=int, default=5,\n",
    "                    help=\"Size of images in the grid\")\n",
    "parser.add_argument(\"--row_wise\", type=bool_flag, default=True,\n",
    "                    help=\"Represent image interpolations horizontally\")\n",
    "parser.add_argument(\"--output_path\", type=str, default=\"output.png\",\n",
    "                    help=\"Output path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulating argparse in command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "argv = '--model_path models/default/paoucvpr8a/best_rec_ae.pth --n_images 10 --n_interpolations 10 --alpha_min 2.0 --alpha_max 2.0 --output_path test_eyes.png'.split( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = parser.parse_args(argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(alpha_max=2.0, alpha_min=2.0, model_path='models/default/paoucvpr8a/best_rec_ae.pth', n_images=10, n_interpolations=10, offset=0, output_path='test_eyes.png', plot_size=5, row_wise=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restore default values for some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.debug = True\n",
    "params.batch_size = 32\n",
    "params.v_flip = False\n",
    "params.h_flip = False\n",
    "params.img_sz = ae.img_sz\n",
    "params.attr = ae.attr\n",
    "params.n_attr = ae.n_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male 2\n",
      "Smiling 2\n"
     ]
    }
   ],
   "source": [
    "for (i, n_cat) in params.attr:\n",
    "    print(i, n_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/ubuntu/FaderNetworks/data'\n",
    "images_filename = 'images_%i_%i.pth'\n",
    "images_filename = images_filename % (params.img_sz, params.img_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.load(os.path.join(DATA_PATH, images_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([202599, 3, 256, 256])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = torch.load(os.path.join(DATA_PATH, 'attributes.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parse attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "attrs = []\n",
    "for name, n_cat in params.attr:\n",
    "    for i in range(n_cat):\n",
    "        attrs.append(torch.FloatTensor((attributes[name] == i).astype(np.float32)))\n",
    "        \n",
    "attributes = torch.cat([x.unsqueeze(1) for x in attrs], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([202599, 4])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = 10000\n",
    "valid_index = 15000\n",
    "test_index = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = images[:train_index]\n",
    "valid_images = images[train_index:valid_index]\n",
    "test_images = images[valid_index:test_index]\n",
    "train_attributes = attributes[:train_index]\n",
    "valid_attributes = attributes[train_index:valid_index]\n",
    "test_attributes = attributes[valid_index:test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_images, valid_images, test_images\n",
    "attributes = train_attributes, valid_attributes, test_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = DataSampler(data[2], attributes[2], params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct images / create interpolations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.images.size(0)\n",
    "#images, attributes = test_data.eval_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, attributes = test_data.eval_batch(40, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_outputs = ae.encode(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolation values\n",
    "alphas = np.linspace(1 - params.alpha_min, params.alpha_max, params.n_interpolations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.        , -0.66666667, -0.33333333,  0.        ,  0.33333333,\n",
       "        0.66666667,  1.        ,  1.33333333,  1.66666667,  2.        ])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [torch.FloatTensor([1 - alpha, alpha, 1 - alpha, alpha]) for alpha in alphas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  2\n",
       " -1\n",
       "  2\n",
       " -1\n",
       " [torch.FloatTensor of size 4], \n",
       "  1.6667\n",
       " -0.6667\n",
       "  1.6667\n",
       " -0.6667\n",
       " [torch.FloatTensor of size 4], \n",
       "  1.3333\n",
       " -0.3333\n",
       "  1.3333\n",
       " -0.3333\n",
       " [torch.FloatTensor of size 4], \n",
       "  1\n",
       "  0\n",
       "  1\n",
       "  0\n",
       " [torch.FloatTensor of size 4], \n",
       "  0.6667\n",
       "  0.3333\n",
       "  0.6667\n",
       "  0.3333\n",
       " [torch.FloatTensor of size 4], \n",
       "  0.3333\n",
       "  0.6667\n",
       "  0.3333\n",
       "  0.6667\n",
       " [torch.FloatTensor of size 4], \n",
       "  0\n",
       "  1\n",
       "  0\n",
       "  1\n",
       " [torch.FloatTensor of size 4], \n",
       " -0.3333\n",
       "  1.3333\n",
       " -0.3333\n",
       "  1.3333\n",
       " [torch.FloatTensor of size 4], \n",
       " -0.6667\n",
       "  1.6667\n",
       " -0.6667\n",
       "  1.6667\n",
       " [torch.FloatTensor of size 4], \n",
       " -1\n",
       "  2\n",
       " -1\n",
       "  2\n",
       " [torch.FloatTensor of size 4]]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original image / reconstructed image / interpolations\n",
    "outputs = []\n",
    "outputs.append(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.append(ae.decode(enc_outputs, attributes)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_outputs[0].size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.n_attr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is going on inside the for-loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = alphas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 2\n",
       "-1\n",
       " 2\n",
       "-1\n",
       "[torch.FloatTensor of size 4]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 2 -1  2 -1\n",
       "[torch.FloatTensor of size 1x4]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Variable.cuda of Variable containing:\n",
       "    2    -1     2    -1\n",
       "    2    -1     2    -1\n",
       "    2    -1     2    -1\n",
       "    2    -1     2    -1\n",
       "    2    -1     2    -1\n",
       "    2    -1     2    -1\n",
       "    2    -1     2    -1\n",
       "    2    -1     2    -1\n",
       "    2    -1     2    -1\n",
       "    2    -1     2    -1\n",
       "[torch.FloatTensor of size 10x4]\n",
       ">"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Variable(alpha.unsqueeze(0).expand((10,alpha.size(0)))).cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in alphas:\n",
    "    alpha = Variable(alpha.unsqueeze(0).expand((len(images), alpha.size(0))).cuda())\n",
    "    outputs.append(ae.decode(enc_outputs, alpha)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = torch.cat([x.unsqueeze(1) for x in outputs], 1).data.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 12, 3, 256, 256])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolations = torch.cat(result, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120, 3, 256, 256])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolations.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 12, 3, 256, 256)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.n_images, 2 + params.n_interpolations, 3, params.img_sz, params.img_sz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert result.size() == (params.n_images, 2 + params.n_interpolations,\n",
    "                                 3, params.img_sz, params.img_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid(images, row_wise, plot_size=5):\n",
    "    \"\"\"\n",
    "    Create a grid with all images.\n",
    "    \"\"\"\n",
    "    n_images, n_columns, img_fm, img_sz, _ = images.size()\n",
    "    if not row_wise:\n",
    "        images = images.transpose(0, 1).contiguous()\n",
    "    images = images.view(n_images * n_columns, img_fm, img_sz, img_sz)\n",
    "    images.add_(1).div_(2.0)\n",
    "    return make_grid(images, nrow=(n_columns if row_wise else n_images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = get_grid(result, params.row_wise, params.plot_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.output_path = 'gender_smile.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.image.imsave(params.output_path, grid.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
